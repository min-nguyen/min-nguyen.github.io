---
title: 'ESOP22 - (Nick, Meng, Roly): Meeting 1'
date: 2021-11-08
permalink: /meetings/2021/11/esop22-1/
---

Explaining Context to NW:

We are developing a formalism/calculus to describe the implementation behind Wasabaye, in order to have enough of a formal setting to begin thinking about some of the metatheory behind the language. Hence it may make sense that some of the points we remark about the implementation paper may be better addressed in a follow-up paper.

With respect to the implementation paper, the majority of the time was spent on the first half of the paper (before the inference effect handler section) in order to reverse-engineer the implementation process. We would like more information on what is the kind of metatheory that might surround this sort of language.

NW: This is helpful context. Initial impression was that the ESOP submission was a very heavy implementation paper: it's neat, but as a reviewer, why trust it or the foundations of it. But given the context surrounding the research overall, I can see why you've written it as you have, as it is much more application/implementation focused.

MW: How are similar papers (probabilistic programming papers, e.g. Monad Bayes) presented?

MN: They tend to be mostly implementation, but also make references to theory and proofs, which we haven't got any of at the moment.

NW: This sounds like a missing ingredient that at the moment seems like it would go in the more theory-like paper? Ideally, the paper order would be the other way round. If the theory paper come first, then the implementation paper would be much less arduous as it could to point the theory paper without going into much detail, thus also providing a confidence behind the implementation. A reviewer looking at the ESOP implementation paper submission may be suspicious about what foundations we are resting on and reject it on those grounds, in which case, I would personally respond by putting the implementation paper on ice for a while, write the theory paper, and then come back to the implementation paper allowing us to omit quite a bit of existing detail such as how the language is constructed but instead talk more about the interface, and have various case studies about we can do with them and how it compares to other languages.

RW: If its accepted, then we perhaps would need to introduce a slightly different spin that discusses the metatheory we might expect to have, hence foreshadowing the theoretical paper. If it isn't accepted, then the way to do things would probably be to push on the theoretical paper first with a slightly different starting point.

MW: A potential problem is that we don't want to get too much into performance, which is what real implementation papers can't avoid.

NW: Whilst performancing is difficult to achieve (we don't want to compete against well-established, high performing PPLs), we can get into the correct performance ball park with not too much engineering, just by using the usual tricks e.g. continuations, cayley representation, perhaps some fusion. I don't see why we wouldn't be competitive with ordinary Haskell code.

RP: Another important dimension of the implementation paper can be the user-experience aspect of it -- we're not necessarily providing an implementation where the focus is performance, but rather compositionality and models. We can emphasize this with nice examples, and then the performance aspect is something people may be willing to suffer through for these nice compositional benefits, which seems like a reasonable trade-off.

NW: A reviewer which comments that our performance is bad compared to, for example tensorflow, is easy to rebut: we can state that this is not our goal, and want to establish first a proof-of-concept that reviewers are happy with before we invest time into efficiency. Taking a step back, the battle plan for what we do with the paper really depends on what the reviewers do with it. Perhaps we should discuss ideas behind the theoretical paper.

RP: What we're trying to do in the formalism is capture the embedding approach behind the implementation paper; the idea is to describe a minimal calculus which has enough support for algebraic effects in order to be able to use the embedding approach that has been implemented, but where the calculus has a simpler formal setting than that of Haskell. Then there's a separate implicit step where we go from this calculus with algebraic effects into a language such as Haskell where we would need to pick an encoding for algebraic effects. We're basing our calculus primarily on Sam Lindley's work.

NW: So the story you're trying to tell is not so much about modularity of algebraic effects, but what happens when we freeze on a specific subset of effects and try to specialise the calculus.

RP: That's a fair way to characterise it.

NW: Then this begs the question "why algebraic effects?". The algebraicity properties are obviously very nice for the predictability of the code, so this is one aspect.

RP: This is where we don't have a clear picture. If we understood what the metatheory was that we wanted to establish, that would be help. What do you think in that approach?

NW: From a sceptical point, be careful not to reinvent the algebraic effects wheel all over again but an impoverished version which is too specialised; then we would lose the whole point of algebraic effects. The point seems to be to have extensible and reinterpretable effects. The immutability is an important part of paper, so that's solid. If you're sacrificing on the modularity, that might not be so important if you don't think you're going to have to host a variety of new and different effects as you go.

RP: Our formalism would like to have enough generality to support ideas such as modular inference algorithms which can be implemented by providing specialised effect handlers and potentially rewriting of the program to have specialised effects that weren't present in the original program.

***

MW: What kind of properties are you imagining of the language?

NW: So you're drawing random values, so we have randomisation going on. Another property is non-determinism -- will there be non-determinism going on i.e. are we choosing from different samples? The next question could be how will probability and non-determinism interact, which is a difficult question (probability and non-determinic choice do not necessarily distribute). True non-determinism is providing a set of possible worlds, any of which can be chosen. This is different from picking a certain world with a certain probability. With non-determinism, we could offer different interpretations of what this means, e.g. one could be "here are 4 different answers and I'm going to tell you what you get", another could be "here are 4 different answers and you get to choose which one you like". If we have probability as an effect working with non-determinism, suppose we pick a randomly distributed value from somewhere, the question then becomes "does this value propagate into the 4 different worlds, or do we need to draw 4 randomly distributed values from those different worlds?". The answer to this question depends on how we want to distribute our probability tree through non-determinism. That then opens up a lot of different avenues and applications - one example is efficiency for instance, e.g. "do we need to maintain 4 copies of the state, or is it enough to maintain one copy of the random value generator?". It could be that we want to do some concurrency in our language, in which case, how do we model that aspect. It could be that the model requires us to say "one of two things happens", and we're not sure which it is. It depends what kinds of problems we want to model. We could also talk about how probability interacts with state, and the same questions arise: once we've picked a random variable and we're modifying some kind of state, and then we pick another random variable, is there a dependency between the state and random variable or not, or is it that we have a global state on the outside and what goes on in the inside doesn't matter. Again, we're asking the same question of how does state distribute through probability.

RP: Is this related to how monads distribute over each other?

NW: Exactly, and that is connected to something we call "distributive laws" which are natural transformations between two different monads. So if we have the monad for probabilism and the monad for state, then can we identify the distributive laws: if we have state as `s` and probability as `p`, we're trying to distribute `s . p` to `p . s` or the other way round. Identifying that distributive law pins down the semantics that we're trying to give to algebraic effects. Another way to say that is, we can imagine different ways of implementing this; we can provide the handler for state and the handler for probability, and depending on the order which we execute these, we may end up changing the semantics of the program. Maybe the point is that we want to give the end user the choice of doing both of these things, or maybe we need a proof that shows it doesn't matter what they chose and so we can optimise things. These are all theoretical concerns that we need to dig into, and these are decisions that need to be made about how the language should behave.

RP: If we had a distributive law between two effects, would this potentially establish that it didn't matter what order we did the handling in?

NW: It's not that it doesn't matter, it's that it is determined what we want to do i.e. it specifies the behaviour between two effects. If the distributive law is one-way, then it specifies that something in particular will happen when we choose to execute one particular handler after another. If the distributive law is an isomorphism, then we can distribute the handlers either way, in other words executing one handler followed by the other doesn't change the semantics because isomorphically it is the same thing. These are the kind of observations that we need to make, for example, what happens when we raise exceptions inside a probabilistic world. There is a whole category of these things that we need to go through and work out what we want: do we want to give these behaviours flexibility or do we want to specify them. All of these things are things we would have to think about anyway if we don't go down the algebraic effects setting, because when defining a language with probabilism, it's very naive to think that there aren't other effects that going to come into play -- usually these effects just get shoved into the side and someone works that out later on, whereas here we're being very upfront about "there are effects in this world, what are we going to do with it?".

RP: So algebraic effects gives us enough of a setting to tackle these issues head-on rather than pushing them away which may come back to bite us?

NW: Exactly, they bite you up-front because we are so explicit about everything that happens. We start thinking with a pure base, and think "if there's anything we want to add, what is it going to be". Even very pragmatically, things like file handling: if we are opening a file to get some input, how does this interact with probability? Does it mean that with some probability that we open a file, or does it mean that we open a file upfront at the beginning of everything? And obviously its the latter, but again, we need to think about how all of these things interact.

MN: So what are we trying to show -- is it just how probabilistic effects interact with every other effect, or?

NW: From reading the paper that you have, the research interest is that you're trying to design a DSL for probabilistic effects/models. I wouldn't try to be super general and answer the question "how do all effects interact with everything else", that's too large a topic. If i were starting from scratch, I would want to do a survey of other probabilistic languages and what kind of language features they have that we would want to support, e.g. do they bother with non-determinism at all, do they have IO, what are their approaches to these different things and what are the trade-offs that they've made, and this will help us understand the kind of effects that we want to model and how we are supposed to do it informed with all of this theory that we have.

RP: If we take compositional inference algorithms to be a requirement, and we have a minimal amount of features that we know those algorithms are going to need, then that gives us a minimal amount of effects that we'd need to consider the interactions of this.

NW: A good starting point for the general theory you'd need to like into is the paper [Reasoning about Effect Interaction by Fusion (ICFP '21) - Zhixuan Yang, Nicolas Wu](https://yangzhixuan.github.io/pdf/fused-reasoning-appendices.pdf) which talks about the fusion of effects, distribution, and how laws interact then.

RP: This is also a relevant paper: [Distributive interaction of algebraic effects](https://ora.ox.ac.uk/objects/uuid:66106628-0a71-4564-bc34-c398db766818).

MW: NW, are you therefore saying, that no matter the way we are doing a particular application this, it always comes down to the specific effects that the problem supports, and the general expectation that we need to reason about interactions with other effects?

NW: Yes, the algebraic effects story has been so far focused on the very generalised story about how effects interact in general. This is a great case study on designing a language with algebraic effects in mind from the ground up, and it's usefulness will become not just a study of algebraic effects in their own right, but how do we go about designing a DSL with specific desired requirements. If we've learned anything about effects, it's "we'll understand _these_ specific interactions first, compose them, and then we're done" -- that's the hope. So it would be mistake to think about how every possible effect in the world would interact. It's better to state our requirements, and then think "how do these things interact", look at how the operations from different families of effects behave, and then specify that concretely with how the handleres interact.

From a software engineering perspective, we could imagine two different effects for example state and probability. We could implement these two as two separate algebraic effects, one specifying state and one specifying probability, and then the choice in the order of composing their handlers will change the semantics that we have. The other approach is to handle both effects at once; this is not compositional, but if you don't need the compositionality, it opens the possibility to having optimisations for free -- we don't have to rely on interesting fusion properties to make those handlers interact nicely, maybe we can handweave something that is better. Although the whole fusion perspective is that compositionality is king, in practice it's not always needed as perhaps we would strive into something that is a bit too difficult to look for fusion in.

MW: I'm surprised that this has not been done -  have there been these kinds of demonstrations of the usage of algebraic effects?

NW: Demonstrations have always been done on the level of a generalised language for effects where the end-user specifies what to do with different effects in different contexts. This is different to designing a bespoke DSL with algebraic effects in mind so that you the designer can more quickly come to the final implementation that you want. I.e. the former is a generous approach: how do we provide general effects and make those interact in interesting ways; the latter is a specialised build for a specialised purpose. We could blindly carry on developing a language for embedded probabilistic models, but we should be using our experience on algebraic effects to help design this, and that doesn't  force us to use the generous approaches actually, it's better to show that sometimes specialisation is a useful thing, and i think that's where we can go here. And this idea hasn't been done. We could argue that Pyro have gone in this direction as they've been lightly inspired by algebraic effects, but my inclination is to stick quite close to the theory and specialise when needs be hence being more heavily inspired by algebraic effects. This then goes into the theory aspect that we were talking about earlier on, i.e. what is the calculus behind this and how does it work out. All these are interesting questions and that to me is the value in this work.

NW:  Most urgent priority to me is to do a survey of current probabilistic languages with the aim to understand what effects they use and how these effects interact - this will become the background behind whatever paper you write about this topic when designing your own language. I can also imagine this being a table in your own paper expressing here are the features of other languages, and here is why we decided to use the features in our language.

***

NW: I think the ESOP paper straddles a little bit into the theory world, but not whole-heartedly, which makes it a bit confusing. I also don't think it's full tilt into the implementation world either. If i were to redirect, I would say the current paper would be much stronger after the theory paper. I could imagine this paper going to a much more applied conference where we're not going to see so much reasoning about the definition of kleisli composition or folding etc, but just what the end result is. In other words, how do we put the pieces together rather than what the pieces are made of.

NW: I think the paper is more applied than ESOP would generally be, but not applied enough for a software engineering paper.

MN: I like the idea of an embedding approach to the paper.

NW: I think that the embedding story could be fleshed out in an ICFP audience type paper, where embeddings themselves are the object of study, and this is a case study of that. In which case, you would go more easy on the implementation side and more harder on embeddings and what that means etc. That's what I mean by the current paper is in a sort of no-man's land.

If you were to make this an embedding paper, then I'm not seeing enough definitions of what the model is to start with and where that is grounded, and I'd like to see the reconstruction of how that is made from the beginning. Whereas here, you're showing it by example with use-cases. In the section "Embedding models as algebraic effects" is where you really get running. I feel that at that point (8 pages in), it is too late to have an embedding story, so it feels that embedding is not the focus of the paper. In an embedding version of the paper, I would ask the question "what are the challenges you are trying to solve? why is embedding necessary or difficult? why is this the right way to approach this problem? what would we gain in this approach?"; for an ICFP audience, I would be much more interested in that kind of story -- "why is this model type the right choice in the first place, where does it come from, and what are its properties?".

RP: Question w.r.t the general algebraic effects approach: say we're using this approach to implement one of these modular inference algorithms as a bunch of handlers, how do you go about checking that the code is correct w.r.t to the things that we're giving semantics to. E.g. if there's a monad we're interpreting, is there an implication that follows from the monad laws that constraints what you implement.

NW: Yes, the key point is that you need to make sure that the operations are algebraic, i.e. the bind distributions through them appropriately. This is sort of enforced in the Haskell world in the way that signatures are functorial. Giving algebras for these functors then ensures that we respect algebraicity.

The way algebraic effects works is that you specify your syntax, and then you apply handlers to that syntax. The way this happens in Haskell is that we specify our syntax as data types, and you implement some handlers around those things, and you should be done. But in practice, there is nothing stopping you from having different operations and handlers in different interleavings; for example, you can imagine having a piece of code that is handled by something which then produces a new piece of code which is then going to have operations around it which is then all handled by something else, and so on. And now we're playing with fire, because this is not what algebraic effect handlers is supposed to do. What you're supposed to have is this clean delineation between all operations as syntax in one place and all semantics in another place: so all our trees get constructed, and we have a chain of handlers uninterrupted with no operations between. Its okay for those operations to generate new syntax trees, as long as there no interleavings of new operations around handlers. When we start putting operations inbetween handlers, you can violate the algebraicity rules, and this is what the whole "scoped effects" work is about: it's about when are algebraicity rules violated, what goes wrong, and how to fix it. An example of this is the `try-catch` example which is classic for algebraic effects. The `catch` is not an operation, it is a handler. What this means is that if we have a piece of code and we're doing a `catch` around that and we continue with a computation, then that `catch` cannot be surrounded by further operations, otherwise we're interleaving operations and handlers, because `catch` is a handler and if we're following that with a `get` operation and then we do some stuff, then we've got operations interleaved with our handler. What goes wrong is that we can get strange interference between the program being interpreted on the inside and the program that we've generated on the outside. We might imagine that there is an outer handler trying to interpret all the effects on the inside of a program, but we may see that we end up violating some algebraicity properties, ending in strange behaviour. Scoped effects are about how to do it safely (which is another can of worms). Reading for this as a more implementation focused paper is [Effect Handlers in Scope (Haskell '14) - Nicolas Wu, Tom Schrijvers, Ralf Hinze](https://dl.acm.org/doi/pdf/10.1145/2633357.2633358?casa_token=iURV1V2DufcAAAAA:Hm2F4g0AYO0IZf178uolQ0cMFnZakKuxznOjBsYHDRq-4ByVqvnykgOEoFn-e-wTf8WgkbuIdsrH6w), and then there's a more theoretical one called [Syntax and Semantics for Operations with Scopes (LICS '18) - Maciej Pirog, Tom Schrijvers, Nicolas Wu](https://dl.acm.org/doi/pdf/10.1145/3209108.3209166).

RP: I'm not sure whether MN is using this pattern in the implementation of inference algorithms, I'm guessing that these would let us know whether some of these algorithms require us to do stuff like this (effects in scope), although the papers suggested will also tell us how to always reformulate the implementation as a clean separation of syntax and handlers that obeys the algebraicity rules.

NW: Yes

MN: As i understand it, we're looking to understand how the necessary effects in our language interact, and proving algebraicity properties of our language.

NW: Yes, if i were you, i would see what effects are already in play in existing probabilistic languages. List the languages, list the effects that they use, and think about how these effects interact with the language. For example, is it always state on the outside, or does state sometimes go on the inside, or do exceptions go on the outside, and in general how do these things distribute. And by even just doing this survey, it will be quite a useful exercise. You may notice that everyone does things a certain way, and why is that in this case? Is it because it's the right way or because it's the easy way. Or you may find that approaches differ in certain aspects. Try to see what the distribution of how things are done is, and this will inform our choices about what we want to do.

RP: When we talk about probability as an effect, are you talking about the probability monad?

NW: Yes, what i mean by that is, there would be some operation such as `random` that gives you back a randomly generated value, and lets us access a probability distribution.

***

RP: We have various primitive distributions represented as constants i.e. data types. How would you see that fitting into the probability monad.

MN: That's a good point because I'm using external statistics libraries for sampling, so I don't really know how the probability monad comes in.

NW: The tldr is that it doesn't matter, it can all be working within the probability monad. The specifics is that if all you provide is a uniform distribution, then you can reconstitute other distributions from that by using ordinary functions. And then the question of "which monad are we working in?" becomes the question of "are we able to have uniformly distributed varibles?", and if we are, then everything comes out of that. The way you do uniformly distributed values is to take two uniformly distributed numbers between 0 and 1, and then you plot those on a circle with an x and y axis, and because we plot on a circle, the angles that you have give rise to two independent normal distributions, and then you can sample from those two distributions off the back of what you've done. This is just one example, but you can do this with beta distributions, gamma distributions etc. This is from the theory perspective i.e. it's enough to have a uniform distribution. From the practical perspective, you might be better off calling from some external C library to get back a result quickly. I would say that this actually goes against the grain of what we're trying to do. To me, what is interesting about probabilistic programming, is that when we're modelling the behaviour of some probabilistic system, what we're really doing is generating a new very specialised probability distribution for that system. So if we can't even build our own nuts and bolts (beta distributions, gamma distributions etc) from other smaller nuts and bolts, then we're in the wrong business.

RP: That's an interesting point. It's almost like it would be a really good exercise to try and build these nuts and bolts ourselves, it would help our understanding of it, and we may choose to provide an optimisation choice where we instead swap in a C library for these in the actual implementation.

NW: If there's anything we've learned about how programs optimise, it is that by providing algebras, we facilitate the fusion of algebraic steps into one larger step, where internal optimisations can happen within loop cycle. So if we're robbing ourselves of all of the information about say, how a distribution is built of these smaller parts, then we're removing the ability to see some kind of optimisation that lets us step in and pull out some structure, and use it in an unusual way that lets us optimises.

RP: So we're turning every basic distribution into a black box essentially whereas some of these optimisations require insight into the implementation.

NW: Exactly. A spin-off of this project could be to identify scenarios where this is interesting. E.g. we want to have two uniformly distributed values, and it could be more efficient to do something under the hood because of independence. Here's an example: given two uniform values, we can produce two normally distributed values that are independent. Imagine in your code that you are calling from two different normally distributed values, well the black box approach would be to roll four dice to get four uniformly distributed values and make the two normally distributed values. An optimisation may be to realise that we already have two uniform values, so we only need to roll two uniformly distributed dice to get that.

RP: Having a native implementation of distributions could be helpful.

NW: The bread and butter of this library should be combining probabilistic values in interesting ways. If we're not introspecting our own syntax and performing optimisations off the back of that, then we've missed a huge trick, because this syntax is ripe and ready to be exposed -- it is algebraic effects after all, and we have got the syntax in our hands.

MN: Could you summarise what would be a good idea to do w.r.t what you've been talking about.

NW: A good idea could be a dive into probability theory, and asking yourself something like, how can we efficiently do probability and draw probabilistic values from interesting distributions. For example if we want to draw two normally distributed values one after another independently, under the hood what may happen is you would actually draw two uniform values for the first value and two uniform values for the second value and then carrying on. But we happen to know that we've actually generated four normally distributed values there, but we've discarded two of them, and this would be a huge waste in terms of efficiency. We could investigate if there is a way to recover efficiency knowing what the structure of what everything actually is.

MN: Could you elaborate on what you meant by being able to represent any distribution as a uniform distribution?

NW: This is called the [Box-Muller transform](https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform), which is a standard way of making a normal distribution from a uniform distribution. It is sufficient for it to be of mean 0 and standard deviation 1 because we can scale it. The way we do this is to find two uniformly distributed values between 0 and 1, project these onto a circle, do some maths, and we get two independent normally distributed values from that. If we wanted to get two independent normally distributed values, one way is to perform this algorithm twice (using in total four uniformly distributed values), but a better way would be to perform the algorithm once and use both normally distributed values.

MN: Is this transform specific to a uniform and normal distribution relationship?

NW: Yes but it's not as specific as you might imagine. When we start doing things such as beta and gamma distributions, similar things happen. This would be as an orthogonal paper, which is an optimising probability story. One way to tell the first story is "here are some applications and here are some programs that fulfill those applications", another story is "here are some complex probabilistic distributions, and here is how we've managed to do some optimisations behind the scenes because we know how these probabilitic values are being drawn.

MN: Is this relevant to the probability monad?

NW: Yes.

RP: It would be useful to recast some primitive distributions as uniform distributions.

MN: Any nice resources on implementing distributions as other distributions? I don't actually know how distributions are implemented.

NW: All distributions end up being built off of smaller mathematical functions. Some basics from [Biological Sequence Analysis](http://www.mcb111.org/w06/durbin_book.pdf) was a good start and how to use the kind of algorithms we want to use, and this book is a good book on bayesian theory [Bayesian Theory C: 316 (Wiley Series in Probability and Statistics)](https://www.amazon.co.uk/Bayesian-Theory-Wiley-Probability-Statistics/dp/0471924164/ref=sr_1_38?crid=CBGVV27BC3BT&keywords=bayesian&qid=1636107529&sprefix=bayesian+%2Caps%2C61&sr=8-38).
