---
title: 'ESOP22 - (Nick, Meng, Roly): Meeting 1'
date: 2021-11-08
permalink: /meetings/2021/11/esop22-1/
---

Explaining Context to NW:

MN and RP are developing a formalism/calculus to describe the implementation behind Wasabaye, in order to have enough of a formal setting to begin thinking about some of the metatheory behind the language. Hence it may make sense that some of the points we remark about the implementation paper may be better addressed in a follow-up paper.

With respect to the implementation paper, the majority of the time was spent on the first half of the paper (before the inference effect handler section) in order to reverse-engineer the implementation process. We would like more information on what is the kind of metatheory that might surround this sort of language.

NW: This is helpful context. Initial impression was that the ESOP submission was a very heavy implementation paper: it's neat, but as a reviewer, why trust it or the foundations of it. But given the context surrounding the research overall, I can see why you've written it as you have, as it is much more application/implementation focused.

MW: How are similar papers (probabilistic programming papers, e.g. Monad Bayes) presented?

MN: They tend to be mostly implementation, but also make references to theory and proofs, which we haven't got any of at the moment.

NW: This sounds like a missing ingredient that at the moment seems like it would go in the more theory-like paper? Ideally, the paper order would be the other way round. If the theory paper come first, then the implementation paper would be much less arduous as it could to point the theory paper without going into much detail, thus also providing a confidence behind the implementation. A reviewer looking at the ESOP implementation paper submission may be suspicious about what foundations we are resting on and reject it on those grounds, in which case, I would personally respond by putting the implementation paper on ice for a while, write the theory paper, and then come back to the implementation paper allowing us to omit quite a bit of existing detail such as how the language is constructed but instead talk more about the interface, and have various case studies about we can do with them and how it compares to other languages.

RW: If its accepted, then we perhaps would need to introduce a slightly different spin that discusses the metatheory we might expect to have, hence foreshadowing the theoretical paper. If it isn't accepted, then the way to do things would probably be to push on the theoretical paper first with a slightly different starting point.

MW: A potential problem is that we don't want to get too much into performance, which is what real implementation papers can't avoid.

NW: Whilst performancing is difficult to achieve (we don't want to compete against well-established, high performing PPLs), we can get into the correct performance ball park with not too much engineering, just by using the usual tricks e.g. continuations, cayley representation, perhaps some fusion. I don't see why we wouldn't be competitive with ordinary Haskell code.

RP: Another important dimension of the implementation paper can be the user-experience aspect of it -- we're not necessarily providing an implementation where the focus is performance, but rather compositionality and models. We can emphasize this with nice examples, and then the performance aspect is something people may be willing to suffer through for these nice compositional benefits, which seems like a reasonable trade-off.

NW: A reviewer which comments that our performance is bad compared to, for example tensorflow, is easy to rebut: we can state that this is not our goal, and want to establish first a proof-of-concept that reviewers are happy with before we invest time into efficiency. Taking a step back, the battle plan for what we do with the paper really depends on what the reviewers do with it. Perhaps we should discuss ideas behind the theoretical paper.

RP: What we're trying to do in the formalism is capture the embedding approach behind the implementation paper; the idea is to describe a minimal calculus which has enough support for algebraic effects in order to be able to use the embedding approach that has been implemented, but where the calculus has a simpler formal setting than that of Haskell. Then there's a separate implicit step where we go from this calculus with algebraic effects into a language such as Haskell where we would need to pick an encoding for algebraic effects. We're basing our calculus primarily on Sam Lindley's work.

NW: So the story you're trying to tell is not so much about modularity of algebraic effects, but what happens when we freeze on a specific subset of effects and try to specialise the calculus.

RP: That's a fair way to characterise it.

NW: Then this begs the question "why algebraic effects?". The algebraicity properties are obviously very nice for the predictability of the code, so this is one aspect.

RP: This is where we don't have a clear picture. If we understood what the metatheory was that we wanted to establish, that would be help. What do you think in that approach?

NW: From a sceptical point, be careful not to reinvent the algebraic effects wheel all over again but an impoverished version which is too specialised; then we would lose the whole point of algebraic effects.